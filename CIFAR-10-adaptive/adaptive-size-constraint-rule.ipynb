{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# transformations for the test dataset\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# load CIFAR-10 test dataset\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# define a function to unnormalize and convert the image back to a NumPy array\n",
    "def unnormalize_image(image_tensor):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    image = image_tensor.numpy().transpose((1, 2, 0))  # convert from (C, H, W) to (H, W, C)\n",
    "    image = std * image + mean  # unnormalize\n",
    "    image = np.clip(image, 0, 1)  # clip to valid range [0, 1]\n",
    "    return image\n",
    "\n",
    "# plot image with its label distribution\n",
    "def plot_image(image_tensor, label):\n",
    "    # unnormalize and convert image to NumPy format\n",
    "    image = unnormalize_image(image_tensor)\n",
    "    \n",
    "    # plot image and label distribution\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[label])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# show examples for specific indices\n",
    "for i in range(2): \n",
    "    image_tensor, label = testset[i]\n",
    "    plot_image(image_tensor,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# model\n",
    "net = EfficientNetB0()\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "weights_path = \"weights/EfficientNetB0_0.1_100_512_SGD_1\" \n",
    "\n",
    "# load weights into the model\n",
    "net.load_state_dict(torch.load(weights_path))\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_size = len(testset)\n",
    "calibration_size = 1000\n",
    "final_test_size = len(testset) - calibration_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary search parameters\n",
    "tolerance = 0.005\n",
    "epsilon = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used to compute alpha_loo\n",
    "def compute_alpha_i(C, calibration_logits, calibration_scores, i):\n",
    "\n",
    "    sum_label = sum(calibration_scores[j] for j in range(len(calibration_scores)) if j != i)\n",
    "\n",
    "    rs = []\n",
    "    with torch.no_grad():\n",
    "        for k in range(10):\n",
    "            true_label_tensor_random = torch.tensor([k], dtype=torch.long).to(device)\n",
    "            S = criterion(calibration_logits[i], true_label_tensor_random).item()\n",
    "            ratio = (calibration_size + 1) * S / (sum_label + S)\n",
    "            rs.append(ratio)\n",
    "\n",
    "    left, right = epsilon, 1.0 - epsilon\n",
    "    while right - left > tolerance:\n",
    "        alpha = (left + right) / 2\n",
    "        threshold = 1 / alpha\n",
    "\n",
    "        conformal_set = [k for k, ratio in enumerate(rs) if ratio < threshold]\n",
    "\n",
    "        if len(conformal_set) <= C:\n",
    "            right = alpha\n",
    "        else:\n",
    "            left = alpha\n",
    "\n",
    "    alpha_min = (left + right) / 2\n",
    "    return alpha_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "# load pretrained ResNet18\n",
    "extractor = resnet18(pretrained=True)\n",
    "extractor.fc = nn.Identity()  # remove final classification layer\n",
    "extractor.eval()\n",
    "\n",
    "# function to extract features using ResNet18\n",
    "def extract_features(X):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for x in X:\n",
    "            feat = extractor(x.unsqueeze(0)) \n",
    "            features.append(feat)\n",
    "    return torch.cat(features, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entropies_to_discrete_sizes(entropies, min_entropy, max_entropy, max_size=3, min_size=1):\n",
    "    num_bins = max_size - min_size + 1\n",
    "    scaled = np.linspace(0, 1, num_bins) ** (1/2)\n",
    "    bins = min_entropy + (max_entropy - min_entropy) * scaled\n",
    "    return np.digitize(entropies, bins, right=True) + min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# for iteration\n",
    "proba = 0\n",
    "alphas_mins = [] # store the true alpha\n",
    "alphas_loos = [] # store the leave-one-out estimates of alpha\n",
    "Niter = 200\n",
    "set_sizes = []\n",
    "\n",
    "for iter in range(Niter):\n",
    "    print(f\"Iteration {iter}\")\n",
    "\n",
    "    # split the test set\n",
    "    calibration_set, final_test_set = random_split(\n",
    "        testset,\n",
    "        [calibration_size, final_test_size],\n",
    "        generator=torch.Generator().manual_seed(iter)\n",
    "    )\n",
    "\n",
    "    calibration_scores = [] # store the scores\n",
    "    calibration_logits = [] # store the logits (features X)\n",
    "\n",
    "    # compute calibration scores\n",
    "    with torch.no_grad():\n",
    "        for idx in range(calibration_size):\n",
    "            x_sample, y_true = calibration_set[idx]\n",
    "            x_sample = x_sample.unsqueeze(0).to(device)\n",
    "            y_true = torch.tensor([y_true], dtype=torch.long).to(device)\n",
    "            logits = net(x_sample)\n",
    "            score = criterion(logits, y_true).item()\n",
    "            calibration_logits.append(logits)\n",
    "            calibration_scores.append(score)\n",
    "    sum_label = sum(calibration_scores)\n",
    "\n",
    "    # sample one random element from the final test set\n",
    "    random_idx = np.random.choice(final_test_size) \n",
    "    x_random, y_random = final_test_set[random_idx]\n",
    "    y_random = int(y_random) \n",
    "    true_label = y_random\n",
    "\n",
    "    x_random_tensor = x_random.unsqueeze(0).to(device)\n",
    "    logits_random = net(x_random_tensor)\n",
    "\n",
    "    model_prediction = torch.argmax(logits_random).item()\n",
    "\n",
    "    X_calib = torch.stack([x[0] for x in calibration_set])\n",
    "    y_calib = torch.tensor([x[1] for x in calibration_set])\n",
    "\n",
    "    X_calib_features = extract_features(X_calib)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    X_calib_pca = pca.fit_transform(X_calib_features)\n",
    "    x_random_features = extract_features(x_random_tensor.cpu())\n",
    "    x_random_sample_pca = pca.transform(x_random_features)  \n",
    "    \n",
    "    # k-NN\n",
    "    k = 20\n",
    "    nn = NearestNeighbors(n_neighbors=k + 1).fit(X_calib_pca)\n",
    "\n",
    "    # local entropies\n",
    "    local_entropies_calib = []\n",
    "    for x in X_calib_pca:\n",
    "        distances, indices = nn.kneighbors([x])\n",
    "        neighbor_labels = y_calib[indices[0][1:]]\n",
    "        counts = np.bincount(neighbor_labels, minlength=10)\n",
    "        probs = counts / counts.sum()\n",
    "        local_entropies_calib.append(entropy(probs, base=2))\n",
    "    local_entropies_calib = np.array(local_entropies_calib)\n",
    "\n",
    "    # local entropy around test sample\n",
    "    distances, indices = nn.kneighbors(x_random_sample_pca)\n",
    "    neighbor_labels = y_calib[indices[0]]\n",
    "    counts = np.bincount(neighbor_labels, minlength=10)\n",
    "    probs = counts / counts.sum()\n",
    "    local_entropy_test = entropy(probs, base=2)\n",
    "\n",
    "    # print(f\"Random test sample index: {idx}, label: {true_label}\")\n",
    "    # print(f\"Local label entropy (k={k}) around test point: {local_entropy_test:.4f} bits\")\n",
    "\n",
    "    min_entropy = np.min(local_entropies_calib)\n",
    "    max_entropy = np.max(local_entropies_calib)\n",
    "    set_size = map_entropies_to_discrete_sizes(local_entropy_test,min_entropy,max_entropy)\n",
    "    # print(f\"Set size = {set_size}\")\n",
    "    set_sizes.append(set_size)\n",
    "\n",
    "    # compute ratio vector\n",
    "    rs = []\n",
    "    for k in range(10): \n",
    "        true_label_tensor_random = torch.tensor([k], dtype=torch.long).to(device)\n",
    "        S = criterion(logits_random, true_label_tensor_random).item()\n",
    "        ratio = (calibration_size + 1) * S / (sum_label + S)\n",
    "        rs.append(ratio)\n",
    "\n",
    "    # compute alpha_min (\\tilde{\\alpha}) using binary search\n",
    "    left, right = epsilon, 1.0 - epsilon\n",
    "    while right - left > tolerance:\n",
    "        alpha = (left + right) / 2\n",
    "        threshold = 1 / alpha\n",
    "\n",
    "        conformal_set = [k for k, ratio in enumerate(rs) if ratio < threshold]\n",
    "\n",
    "        if len(conformal_set) <= set_size:\n",
    "            right = alpha  # try smaller alpha\n",
    "        else:\n",
    "            left = alpha   # need larger alpha\n",
    "\n",
    "    alpha_min = (left + right) / 2\n",
    "    alphas_mins.append(alpha_min)\n",
    "    \n",
    "    # update empirical coverage probability\n",
    "    if true_label in conformal_set:\n",
    "        proba += 1\n",
    "\n",
    "    print(\"Computing LOO estimator...\")\n",
    "    list_alpha_i = []\n",
    "    for i in range(len(calibration_logits)):\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        list_alpha_i.append(compute_alpha_i(set_size,calibration_logits,calibration_scores,i))\n",
    "    alpha_loo = sum(list_alpha_i)/len(calibration_logits)\n",
    "    alphas_loos.append(alpha_loo)\n",
    "\n",
    "        \n",
    "proba = proba/Niter\n",
    "alpha_mean = sum(alphas_mins)/Niter \n",
    "alpha_loo_mean = sum(alphas_loos)/Niter\n",
    "print(f\"Proba = {proba}\")\n",
    "print(f\"1-E[alpha] = {1-alpha_mean}\")\n",
    "print(f\"E[alpha] = {alpha_mean}\")\n",
    "print(f\"E[alpha_LOO] = {alpha_loo_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of 1-alpha\n",
    "alphas = np.arange(0.005,0.505,0.01)\n",
    "\n",
    "fig_alpha, ax_alpha = plt.subplots(figsize=(7, 4)) \n",
    "ax_alpha.hist(1-np.array(alphas_mins), color='green',alpha=0.4,bins=1-np.array(alphas[::-1]),align='left',label=r\"$\\tilde{\\alpha}$\")\n",
    "ax_alpha.hist(1-np.array(alphas_loos), color='blue',alpha=0.4,bins=1-np.array(alphas[::-1]),align='left',label=r\"$\\hat{\\alpha}^{LOO}$\")\n",
    "ax_alpha.axvline(x=proba, color='red', linestyle='--', linewidth=3,label=r\"$Pr(Y_{\\rm test} \\in \\hat{C}_n(X_{\\rm test}))$\")\n",
    "ax_alpha.axvline(x=np.mean(1-np.array(alphas_mins)), color='green', linestyle='--', linewidth=3)\n",
    "ax_alpha.axvline(x=np.mean(1-np.array(alphas_loos)), color='blue', linestyle='--', linewidth=3)\n",
    "ax_alpha.set_xlabel(r\"$1-\\tilde{\\alpha}$\",fontsize=16)\n",
    "ax_alpha.set_ylabel(\"Frequency\",fontsize=16)\n",
    "\n",
    "ax_alpha.set_xlim(0.8,1)\n",
    "ax_alpha.set_ylim(0,105)\n",
    "\n",
    "ax_alpha.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "ax_alpha.set_xticks(np.linspace(0.5, 1.0, 11))\n",
    "ax_alpha.set_xticks(np.linspace(0.5, 1.0, 21),minor=True)\n",
    "\n",
    "ax_alpha.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/hist_n\"+str(calibration_size)+\".pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"alphas/alphas_mins_n\"+str(calibration_size)+\".npy\", alphas_mins)\n",
    "np.save(\"alphas/alphas_loos_n\"+str(calibration_size)+\".npy\", alphas_loos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
